{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfbc7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 1: First Model Training\n",
    "# Cell 1: Imports & Config\n",
    "from pathlib import Path\n",
    "import math, cv2, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Unexpected keys .*\")\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent  \n",
    "\n",
    "GT99_IMG_DIR = PROJECT_ROOT / \"data\" / \"annotation_batch\"\n",
    "GT99_CSV = GT99_IMG_DIR / \"keypoints_normalized_FIXED.csv\"\n",
    "\n",
    "# Hyperparams\n",
    "IMG_SIZE = 512\n",
    "HEATMAP_SIZE = 64\n",
    "SIGMA = 4\n",
    "BATCH_SIZE = 4\n",
    "ACCUM_STEPS = 2     # gradient accumulation\n",
    "LR = 1e-4\n",
    "EPOCHS_STAGE1 = 80  # supervised\n",
    "PATIENCE = 15\n",
    "\n",
    "# Device priority: MPS > CUDA > CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "torch.set_num_threads(4)  # limit CPU threads\n",
    "LOG_PATH = PROJECT_ROOT / \"training_log.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9acfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Utilities\n",
    "def norberg_angle(p1, p2, p3):\n",
    "    a, b, c = np.array(p1), np.array(p2), np.array(p3)\n",
    "    AB, AC = b-a, c-a\n",
    "    dot = np.dot(AB, AC)\n",
    "    norms = np.linalg.norm(AB) * np.linalg.norm(AC)\n",
    "    return 0 if norms == 0 else math.degrees(math.acos(np.clip(dot / norms, -1.0, 1.0)))\n",
    "\n",
    "def generate_heatmaps(keypoints, heatmap_size=HEATMAP_SIZE, img_size=IMG_SIZE, sigma=SIGMA):\n",
    "    num_kps = len(keypoints)\n",
    "    heatmaps = np.zeros((num_kps, heatmap_size, heatmap_size), dtype=np.float32)\n",
    "    for i, (x, y) in enumerate(keypoints):\n",
    "        x_h = x * (heatmap_size / img_size)\n",
    "        y_h = y * (heatmap_size / img_size)\n",
    "        xx, yy = np.meshgrid(np.arange(heatmap_size), np.arange(heatmap_size))\n",
    "        heatmaps[i] = np.exp(-((xx - x_h) ** 2 + (yy - y_h) ** 2) / (2 * sigma ** 2))\n",
    "    return heatmaps\n",
    "\n",
    "def soft_argmax_2d(heatmaps, output_size=IMG_SIZE, heatmap_size=HEATMAP_SIZE):\n",
    "    N, K, H, W = heatmaps.shape\n",
    "    flat = heatmaps.view(N, K, -1)\n",
    "    flat = torch.softmax(flat, dim=-1)\n",
    "    coords_x = torch.arange(W).repeat(H, 1).reshape(-1).float().to(flat.device)\n",
    "    coords_y = torch.arange(H).repeat_interleave(W).float().to(flat.device)\n",
    "    xs = torch.sum(flat * coords_x, dim=-1)\n",
    "    ys = torch.sum(flat * coords_y, dim=-1)\n",
    "    scale_x = output_size / float(heatmap_size)\n",
    "    scale_y = output_size / float(heatmap_size)\n",
    "    coords = torch.stack([xs * scale_x, ys * scale_y], dim=-1)\n",
    "    return coords.view(N, K, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70034a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Dataset\n",
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transforms=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = cv2.cvtColor(cv2.imread(str(self.img_dir / row['image_name'])), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        keypoints = np.array([\n",
    "            (row['L_FHC_x'] * IMG_SIZE, row['L_FHC_y'] * IMG_SIZE),\n",
    "            (row['R_FHC_x'] * IMG_SIZE, row['R_FHC_y'] * IMG_SIZE),\n",
    "            (row['L_CAR_x'] * IMG_SIZE, row['L_CAR_y'] * IMG_SIZE),\n",
    "            (row['R_CAR_x'] * IMG_SIZE, row['R_CAR_y'] * IMG_SIZE),\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        if self.transforms:\n",
    "            # Albumentations will now receive and process a NumPy array\n",
    "            augmented = self.transforms(image=image, keypoints=keypoints)\n",
    "            image, keypoints = augmented['image'], augmented['keypoints']\n",
    "        \n",
    "        # Keypoints are already a NumPy array, just generate heatmaps\n",
    "        heatmaps = generate_heatmaps(keypoints.tolist()) # generate_heatmaps needs a list\n",
    "        return image, torch.tensor(heatmaps), np.array(keypoints, dtype=np.float32), row['image_name']\n",
    "\n",
    "def custom_collate(batch):\n",
    "    imgs, hmaps, kps, names = zip(*batch)\n",
    "    return torch.stack(imgs), torch.stack(hmaps), list(kps), list(names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafef885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryan078/Desktop/CHD_project/venv/lib/python3.8/site-packages/pydantic/main.py:214: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Augmentations\n",
    "class HorizontalFlipWithKeypointSwap(A.DualTransform):\n",
    "    def __init__(self, always_apply: bool = False, p: float = 0.5):\n",
    "        super().__init__(always_apply=always_apply, p=p)\n",
    "\n",
    "    def apply(self, image, **params):\n",
    "        return cv2.flip(image, 1)\n",
    "\n",
    "    def apply_to_keypoints(self, keypoints, **params):\n",
    "        # Flip x-coordinate\n",
    "        keypoints[:, 0] = IMG_SIZE - keypoints[:, 0]\n",
    "        \n",
    "        # Swap L-R pairs using NumPy's advanced indexing\n",
    "        # Swap indices 0 and 1 (L_FHC <-> R_FHC)\n",
    "        keypoints[[0, 1]] = keypoints[[1, 0]]\n",
    "        # Swap indices 2 and 3 (L_CAR <-> R_CAR)\n",
    "        keypoints[[2, 3]] = keypoints[[3, 2]]\n",
    "        \n",
    "        return keypoints\n",
    "\n",
    "    \n",
    "    def get_transform_init_args_names(self):\n",
    "        return ()\n",
    "\n",
    "# define transforms\n",
    "train_tf_no_flip = A.Compose([\n",
    "    A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.05, rotate_limit=7, \n",
    "                       border_mode=cv2.BORDER_CONSTANT, p=0.9),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.GaussianBlur(blur_limit=3, p=0.2),\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], keypoint_params=A.KeypointParams(format='xy'))\n",
    "\n",
    "train_tf_flip = A.Compose([\n",
    "    HorizontalFlipWithKeypointSwap(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.05, rotate_limit=7,\n",
    "                       border_mode=cv2.BORDER_CONSTANT, p=0.9),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.GaussianBlur(blur_limit=3, p=0.2),\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], keypoint_params=A.KeypointParams(format='xy'))\n",
    "\n",
    "val_tf = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], keypoint_params=A.KeypointParams(format='xy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50729bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Multi-task Model\n",
    "class HipNet(nn.Module):\n",
    "    def __init__(self, num_keypoints=4):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\"mobilenetv3_large_100\", pretrained=True, features_only=True)\n",
    "        in_ch = self.backbone.feature_info[-1]['num_chs']\n",
    "        # Keypoint heatmap head\n",
    "        self.kp_head = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, num_keypoints, 1)\n",
    "        )\n",
    "        # Angle regression head\n",
    "        self.angle_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "            nn.Linear(in_ch, 2)  # left & right Norberg angles in deg\n",
    "        )\n",
    "        # CHD classification head\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "            nn.Linear(in_ch, 3)  # Normal, Borderline, CHD\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)[-1]\n",
    "        hmap = self.kp_head(torch.nn.functional.interpolate(feats, size=(HEATMAP_SIZE, HEATMAP_SIZE)))\n",
    "        angles = self.angle_head(feats)\n",
    "        cls_logits = self.cls_head(feats)\n",
    "        return hmap, angles, cls_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce48407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Losses\n",
    "weights = torch.tensor([2.0, 2.0, 1.0, 1.0])\n",
    "def weighted_mse_loss(pred, target):\n",
    "    return (((pred - target) ** 2) * weights.view(1,-1,1,1).to(pred.device)).mean()\n",
    "\n",
    "cls_loss_fn = nn.CrossEntropyLoss()\n",
    "angle_loss_fn = nn.L1Loss()\n",
    "coord_loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91089e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage1][Epoch 1] TrainLoss 202.1872 ValRMSE 75.76\n",
      "[Stage1][Epoch 2] TrainLoss 113.3235 ValRMSE 62.84\n",
      "[Stage1][Epoch 3] TrainLoss 63.0418 ValRMSE 43.15\n",
      "[Stage1][Epoch 4] TrainLoss 37.4805 ValRMSE 32.28\n",
      "[Stage1][Epoch 5] TrainLoss 25.0986 ValRMSE 25.38\n",
      "[Stage1][Epoch 6] TrainLoss 18.6609 ValRMSE 22.33\n",
      "[Stage1][Epoch 7] TrainLoss 13.4232 ValRMSE 18.96\n",
      "[Stage1][Epoch 8] TrainLoss 9.8789 ValRMSE 16.87\n",
      "[Stage1][Epoch 9] TrainLoss 10.2398 ValRMSE 18.48\n",
      "[Stage1][Epoch 10] TrainLoss 7.2755 ValRMSE 16.21\n",
      "[Stage1][Epoch 11] TrainLoss 7.7128 ValRMSE 12.90\n",
      "[Stage1][Epoch 12] TrainLoss 6.1988 ValRMSE 14.62\n",
      "[Stage1][Epoch 13] TrainLoss 5.5694 ValRMSE 16.26\n",
      "[Stage1][Epoch 14] TrainLoss 6.0174 ValRMSE 14.47\n",
      "[Stage1][Epoch 15] TrainLoss 5.1632 ValRMSE 14.13\n",
      "[Stage1][Epoch 16] TrainLoss 4.6335 ValRMSE 17.94\n",
      "[Stage1][Epoch 17] TrainLoss 4.4493 ValRMSE 12.50\n",
      "[Stage1][Epoch 18] TrainLoss 4.3447 ValRMSE 12.72\n",
      "[Stage1][Epoch 19] TrainLoss 3.6946 ValRMSE 14.99\n",
      "[Stage1][Epoch 20] TrainLoss 4.1506 ValRMSE 12.70\n",
      "[Stage1][Epoch 21] TrainLoss 4.2904 ValRMSE 10.68\n",
      "[Stage1][Epoch 22] TrainLoss 4.1832 ValRMSE 10.48\n",
      "[Stage1][Epoch 23] TrainLoss 3.8536 ValRMSE 10.30\n",
      "[Stage1][Epoch 24] TrainLoss 3.7980 ValRMSE 11.28\n",
      "[Stage1][Epoch 25] TrainLoss 3.4424 ValRMSE 12.08\n",
      "[Stage1][Epoch 26] TrainLoss 3.1878 ValRMSE 14.24\n",
      "[Stage1][Epoch 27] TrainLoss 2.9521 ValRMSE 12.14\n",
      "[Stage1][Epoch 28] TrainLoss 2.8931 ValRMSE 12.21\n",
      "[Stage1][Epoch 29] TrainLoss 2.5789 ValRMSE 11.58\n",
      "[Stage1][Epoch 30] TrainLoss 2.7706 ValRMSE 10.92\n",
      "[Stage1][Epoch 31] TrainLoss 3.0613 ValRMSE 10.48\n",
      "[Stage1][Epoch 32] TrainLoss 2.7450 ValRMSE 10.87\n",
      "[Stage1][Epoch 33] TrainLoss 3.0647 ValRMSE 14.73\n",
      "[Stage1][Epoch 34] TrainLoss 3.3309 ValRMSE 9.38\n",
      "[Stage1][Epoch 35] TrainLoss 2.9481 ValRMSE 11.48\n",
      "[Stage1][Epoch 36] TrainLoss 2.7155 ValRMSE 11.88\n",
      "[Stage1][Epoch 37] TrainLoss 2.9437 ValRMSE 11.68\n",
      "[Stage1][Epoch 38] TrainLoss 2.4205 ValRMSE 12.92\n",
      "[Stage1][Epoch 39] TrainLoss 2.4661 ValRMSE 15.62\n",
      "[Stage1][Epoch 40] TrainLoss 2.2284 ValRMSE 10.37\n",
      "[Stage1][Epoch 41] TrainLoss 2.4973 ValRMSE 8.42\n",
      "[Stage1][Epoch 42] TrainLoss 2.1898 ValRMSE 10.37\n",
      "[Stage1][Epoch 43] TrainLoss 2.1760 ValRMSE 12.02\n",
      "[Stage1][Epoch 44] TrainLoss 2.0945 ValRMSE 11.63\n",
      "[Stage1][Epoch 45] TrainLoss 1.9320 ValRMSE 10.00\n",
      "[Stage1][Epoch 46] TrainLoss 1.8898 ValRMSE 11.93\n",
      "[Stage1][Epoch 47] TrainLoss 1.8277 ValRMSE 11.57\n",
      "[Stage1][Epoch 48] TrainLoss 2.0285 ValRMSE 10.25\n",
      "[Stage1][Epoch 49] TrainLoss 1.9490 ValRMSE 9.21\n",
      "[Stage1][Epoch 50] TrainLoss 1.8864 ValRMSE 9.03\n",
      "[Stage1][Epoch 51] TrainLoss 1.9202 ValRMSE 12.03\n",
      "[Stage1][Epoch 52] TrainLoss 1.7503 ValRMSE 13.19\n",
      "[Stage1][Epoch 53] TrainLoss 1.7859 ValRMSE 11.03\n",
      "[Stage1][Epoch 54] TrainLoss 1.8483 ValRMSE 10.74\n",
      "[Stage1][Epoch 55] TrainLoss 2.2897 ValRMSE 8.25\n",
      "[Stage1][Epoch 56] TrainLoss 2.2073 ValRMSE 9.95\n",
      "[Stage1][Epoch 57] TrainLoss 2.0028 ValRMSE 11.71\n",
      "[Stage1][Epoch 58] TrainLoss 1.7628 ValRMSE 8.15\n",
      "[Stage1][Epoch 59] TrainLoss 1.7763 ValRMSE 9.67\n",
      "[Stage1][Epoch 60] TrainLoss 1.6100 ValRMSE 9.99\n",
      "[Stage1][Epoch 61] TrainLoss 1.7292 ValRMSE 8.31\n",
      "[Stage1][Epoch 62] TrainLoss 1.6558 ValRMSE 8.68\n",
      "[Stage1][Epoch 63] TrainLoss 1.5463 ValRMSE 9.16\n",
      "[Stage1][Epoch 64] TrainLoss 1.4826 ValRMSE 8.55\n",
      "[Stage1][Epoch 65] TrainLoss 1.4934 ValRMSE 8.25\n",
      "[Stage1][Epoch 66] TrainLoss 1.5430 ValRMSE 9.00\n",
      "[Stage1][Epoch 67] TrainLoss 1.5275 ValRMSE 8.25\n",
      "[Stage1][Epoch 68] TrainLoss 1.5912 ValRMSE 9.55\n",
      "[Stage1][Epoch 69] TrainLoss 1.4720 ValRMSE 9.27\n",
      "[Stage1][Epoch 70] TrainLoss 1.6231 ValRMSE 9.36\n",
      "[Stage1][Epoch 71] TrainLoss 1.5048 ValRMSE 10.39\n",
      "[Stage1][Epoch 72] TrainLoss 1.3937 ValRMSE 10.17\n",
      "[Stage1][Epoch 73] TrainLoss 1.5296 ValRMSE 8.21\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Stage 1: Supervised Pretrain\n",
    "df_gt99 = pd.read_csv(GT99_CSV)\n",
    "train_df, val_df = train_test_split(df_gt99, test_size=0.2, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(KeypointDataset(train_df, GT99_IMG_DIR, train_tf_no_flip),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(KeypointDataset(val_df, GT99_IMG_DIR, val_tf),\n",
    "                        batch_size=BATCH_SIZE, collate_fn=custom_collate)\n",
    "\n",
    "model = HipNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "best_rmse = float('inf')\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(EPOCHS_STAGE1):\n",
    "    model.train()\n",
    "    if epoch == 20:  # enable flips\n",
    "        train_loader = DataLoader(KeypointDataset(train_df, GT99_IMG_DIR, train_tf_flip),\n",
    "                                  batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)\n",
    "    train_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    for step, (imgs, hmaps, gt_coords, names) in enumerate(train_loader):\n",
    "        imgs, hmaps = imgs.to(device), hmaps.to(device)\n",
    "        hmap_pred, angle_pred, cls_logits = model(imgs)\n",
    "        coords_pred = soft_argmax_2d(hmap_pred)\n",
    "        gt_coords_t = torch.tensor(np.stack(gt_coords), dtype=torch.float32).to(device)\n",
    "        loss_kp = weighted_mse_loss(hmap_pred, hmaps)\n",
    "        loss_coord = coord_loss_fn(coords_pred, gt_coords_t)\n",
    "        loss = loss_kp + 0.05*loss_coord\n",
    "        loss.backward()\n",
    "        if (step+1) % ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_rmse_list = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, _, gt_coords, _ in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            hmap_pred, _, _ = model(imgs)\n",
    "            coords_pred = soft_argmax_2d(hmap_pred).cpu().numpy()\n",
    "            for b in range(len(gt_coords)):\n",
    "                rmse = np.sqrt(((coords_pred[b] - gt_coords[b]) ** 2).sum(axis=1)).mean()\n",
    "                val_rmse_list.append(rmse)\n",
    "    avg_rmse = np.mean(val_rmse_list)\n",
    "    print(f\"[Stage1][Epoch {epoch+1}] TrainLoss {train_loss/len(train_loader):.4f} ValRMSE {avg_rmse:.2f}\")\n",
    "    if avg_rmse < best_rmse:\n",
    "        best_rmse = avg_rmse\n",
    "        torch.save(model.state_dict(), PROJECT_ROOT / \"outputs\" / \"model\" / \"stage1.pth\")\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= PATIENCE: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d89b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fixed Stage 1 train split at /Users/aryan078/Desktop/CHD_project/train_split_gt99.csv\n",
      "Saved fixed Stage 1 validation split at /Users/aryan078/Desktop/CHD_project/val_split_gt99.csv\n"
     ]
    }
   ],
   "source": [
    "#CELL 7-1 saving stage 1 validation split\n",
    "# Read the same annotations used in Stage 1\n",
    "df = pd.read_csv(GT99_CSV)\n",
    "\n",
    "# Match Stage 1's split settings\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paths to save\n",
    "train_path = PROJECT_ROOT / \"outputs\" / \"csv\" / \"train_split_gt99.csv\"\n",
    "val_path = PROJECT_ROOT / \"outputs\" / \"csv\" / \"val_split_gt99.csv\"\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "val_df.to_csv(val_path, index=False)\n",
    "\n",
    "print(f\"Saved fixed Stage 1 train split at {train_path}\")\n",
    "print(f\"Saved fixed Stage 1 validation split at {val_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1155cbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved validation visualizations to: /Users/aryan078/Desktop/CHD_project/val_predictions_vis\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Visualization of Stage 1 Validation Predictions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load fixed validation split\n",
    "val_df = pd.read_csv(PROJECT_ROOT / \"coutputs\" / \"csv\" \"val_split_gt99.csv\")\n",
    "\n",
    "# DataLoader for validation images\n",
    "val_dataset = KeypointDataset(val_df, GT99_IMG_DIR, val_tf)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "# Reload the model and best weights\n",
    "model = HipNet().to(device)\n",
    "model.load_state_dict(torch.load(PROJECT_ROOT / \"stage1.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Directory for saving visualizations\n",
    "VAL_VIS_DIR = PROJECT_ROOT / \"outputs\" / \"viisualizations\" / \"Stage1_final_vis\"\n",
    "VAL_VIS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Loop over validation images\n",
    "with torch.no_grad():\n",
    "    for imgs, _, gt_coords, names in val_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        \n",
    "        # Predict heatmaps & extract coords\n",
    "        hmap_pred, _, _ = model(imgs)\n",
    "        coords_pred = soft_argmax_2d(hmap_pred).cpu().numpy()[0]\n",
    "        coords_gt = np.array(gt_coords[0])\n",
    "        \n",
    "        # Convert tensor image back to NumPy for plotting\n",
    "        img_np = imgs[0].cpu().permute(1, 2, 0).numpy()\n",
    "        img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())  # normalize to [0,1]\n",
    "        \n",
    "        # Plot image with GT and predicted points\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(img_np)\n",
    "        plt.scatter(coords_gt[:,0], coords_gt[:,1], c='g', marker='o', s=40, label='GT')\n",
    "        plt.scatter(coords_pred[:,0], coords_pred[:,1], c='r', marker='x', s=40, label='Pred')\n",
    "        plt.title(names[0])\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save each visualization\n",
    "        out_path = VAL_VIS_DIR / f\"{names[0]}_pred_vs_gt.png\"\n",
    "        plt.savefig(out_path)\n",
    "        plt.close()\n",
    "\n",
    "print(f\"Saved validation visualizations to: {VAL_VIS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c4f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "#STAGE 2: Fine Tuning\n",
    "# Paths\n",
    "GT99_IMG_DIR = PROJECT_ROOT / \"data/annotation_batch\"\n",
    "GT99_CSV = GT99_IMG_DIR / \"keypoints_normalized_FIXED.csv\"\n",
    "VAL_SPLIT_GT99_CSV = PROJECT_ROOT / \"val_split_gt99.csv\"\n",
    "U110_IMG_DIR = PROJECT_ROOT / \"data/u110\"\n",
    "U110_CSV = U110_IMG_DIR / \"keypoints_normalized_from_annotations5.csv\"\n",
    "\n",
    "STAGE1_MODEL_PATH = PROJECT_ROOT / \"outputs\" / \"model\" / \"stage1.pth\"\n",
    "STAGE2_MODEL_PATH = PROJECT_ROOT / \"outputs\" / \"model\" / \"stage2_finetuned.pth\"\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "IMG_SIZE = 512\n",
    "HEATMAP_SIZE = 128\n",
    "SIGMA = 8\n",
    "BATCH_SIZE = 4\n",
    "ACCUM_STEPS = 2\n",
    "LR = 1e-4\n",
    "EPOCHS_STAGE2 = 80\n",
    "PATIENCE = 15\n",
    "WARMUP_EPOCHS = 5\n",
    "\n",
    "# Device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.set_num_threads(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9d0956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 215 | Train: 171 | Val: 44\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "df_gt99 = pd.read_csv(GT99_CSV)\n",
    "df_u110 = pd.read_csv(U110_CSV)\n",
    "\n",
    "# Add absolute image paths\n",
    "df_gt99['image_path'] = df_gt99['image_name'].apply(lambda n: str(GT99_IMG_DIR / n))\n",
    "df_u110['image_path'] = df_u110['image_name'].apply(lambda n: str(U110_IMG_DIR / n))\n",
    "\n",
    "# Validation splits\n",
    "val_gt99_df = pd.read_csv(VAL_SPLIT_GT99_CSV)\n",
    "val_gt99_names = set(val_gt99_df['image_name'])\n",
    "\n",
    "train_u110_df, val_u110_df = train_test_split(df_u110, test_size=0.2, random_state=42)\n",
    "val_u110_names = set(val_u110_df['image_name'])\n",
    "\n",
    "final_val_names = val_gt99_names.union(val_u110_names)\n",
    "df_combined = pd.concat([df_gt99, df_u110], ignore_index=True)\n",
    "final_train_df = df_combined[~df_combined['image_name'].isin(final_val_names)].copy()\n",
    "final_val_df = df_combined[df_combined['image_name'].isin(final_val_names)].copy()\n",
    "\n",
    "# Save splits\n",
    "final_train_df.to_csv(PROJECT_ROOT / \"outputs\" / \"csv\" / \"train_split_combined.csv\", index=False)\n",
    "final_val_df.to_csv(PROJECT_ROOT / \"outputs\" / \"csv\" / \"val_split_combined.csv\", index=False)\n",
    "\n",
    "print(f\"Total: {len(df_combined)} | Train: {len(final_train_df)} | Val: {len(final_val_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c8799d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_heatmaps(keypoints, heatmap_size=HEATMAP_SIZE, img_size=IMG_SIZE, sigma=SIGMA):\n",
    "    num_kps = len(keypoints)\n",
    "    heatmaps = np.zeros((num_kps, heatmap_size, heatmap_size), dtype=np.float32)\n",
    "    for i, (x, y) in enumerate(keypoints):\n",
    "        x_h = x * (heatmap_size / img_size)\n",
    "        y_h = y * (heatmap_size / img_size)\n",
    "        xx, yy = np.meshgrid(np.arange(heatmap_size), np.arange(heatmap_size))\n",
    "        heatmaps[i] = np.exp(-((xx - x_h) ** 2 + (yy - y_h) ** 2) / (2 * sigma ** 2))\n",
    "    return heatmaps\n",
    "\n",
    "def soft_argmax_2d(heatmaps, output_size=IMG_SIZE, heatmap_size=HEATMAP_SIZE):\n",
    "    N, K, H, W = heatmaps.shape\n",
    "    flat = heatmaps.view(N, K, -1)\n",
    "    flat = torch.softmax(flat, dim=-1)\n",
    "    coords_x = torch.arange(W).repeat(H, 1).reshape(-1).float().to(flat.device)\n",
    "    coords_y = torch.arange(H).repeat_interleave(W).float().to(flat.device)\n",
    "    xs = torch.sum(flat * coords_x, dim=-1)\n",
    "    ys = torch.sum(flat * coords_y, dim=-1)\n",
    "    scale = output_size / float(heatmap_size)\n",
    "    coords = torch.stack([xs * scale, ys * scale], dim=-1)\n",
    "    return coords.view(N, K, 2)\n",
    "\n",
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transforms = transforms\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = cv2.cvtColor(cv2.imread(row['image_path']), cv2.COLOR_BGR2RGB)\n",
    "        keypoints = np.array([\n",
    "            (row['L_FHC_x'] * IMG_SIZE, row['L_FHC_y'] * IMG_SIZE),\n",
    "            (row['R_FHC_x'] * IMG_SIZE, row['R_FHC_y'] * IMG_SIZE),\n",
    "            (row['L_CAR_x'] * IMG_SIZE, row['L_CAR_y'] * IMG_SIZE),\n",
    "            (row['R_CAR_x'] * IMG_SIZE, row['R_CAR_y'] * IMG_SIZE),\n",
    "        ], dtype=np.float32)\n",
    "        if self.transforms:\n",
    "            aug = self.transforms(image=image, keypoints=keypoints)\n",
    "            image, keypoints = aug['image'], aug['keypoints']\n",
    "        heatmaps = generate_heatmaps(keypoints.tolist())\n",
    "        return image, torch.tensor(heatmaps), np.array(keypoints, dtype=np.float32), row['image_name']\n",
    "\n",
    "def custom_collate(batch):\n",
    "    imgs, hmaps, kps, names = zip(*batch)\n",
    "    return torch.stack(imgs), torch.stack(hmaps), list(kps), list(names)\n",
    "\n",
    "class HorizontalFlipWithKeypointSwap(A.DualTransform):\n",
    "    def apply(self, image, **params): return cv2.flip(image, 1)\n",
    "    def apply_to_keypoints(self, keypoints, **params):\n",
    "        keypoints[:, 0] = IMG_SIZE - keypoints[:, 0]\n",
    "        keypoints[[0, 1]] = keypoints[[1, 0]]\n",
    "        keypoints[[2, 3]] = keypoints[[3, 2]]\n",
    "        return keypoints\n",
    "    def get_transform_init_args_names(self): return ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1a9e294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_flip = A.Compose([\n",
    "    HorizontalFlipWithKeypointSwap(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.05, rotate_limit=7, border_mode=cv2.BORDER_CONSTANT, p=0.9),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.GaussianBlur(blur_limit=3, p=0.2),\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], keypoint_params=A.KeypointParams(format='xy'))\n",
    "\n",
    "val_tf = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], keypoint_params=A.KeypointParams(format='xy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8aecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HipNet(nn.Module):\n",
    "    def __init__(self, num_keypoints=4):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\"mobilenetv3_large_100\", pretrained=True, features_only=True)\n",
    "        in_ch = self.backbone.feature_info[-1]['num_chs']\n",
    "        self.kp_head = nn.Sequential(nn.Conv2d(in_ch, 128, 3, padding=1), nn.ReLU(),\n",
    "                                     nn.Conv2d(128, num_keypoints, 1))\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)[-1]\n",
    "        hmap = self.kp_head(torch.nn.functional.interpolate(feats, size=(HEATMAP_SIZE, HEATMAP_SIZE)))\n",
    "        return hmap, None, None\n",
    "\n",
    "weights = torch.tensor([2.0, 2.0, 1.0, 1.0])\n",
    "def weighted_mse_loss(pred, target):\n",
    "    return (((pred - target) ** 2) * weights.view(1,-1,1,1).to(pred.device)).mean()\n",
    "\n",
    "coord_loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2219ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Stage 1 weights from /Users/aryan078/Desktop/CHD_project/stage1_best.pth\n",
      "[Epoch 1/80] TrainLoss: 6.2751 | ValRMSE: 10.10px\n",
      "  -> New best model saved! ValRMSE: 10.10px\n",
      "[Epoch 2/80] TrainLoss: 3.6035 | ValRMSE: 10.00px\n",
      "  -> New best model saved! ValRMSE: 10.00px\n",
      "[Epoch 3/80] TrainLoss: 3.4837 | ValRMSE: 9.72px\n",
      "  -> New best model saved! ValRMSE: 9.72px\n",
      "[Epoch 4/80] TrainLoss: 3.7704 | ValRMSE: 9.77px\n",
      "[Epoch 5/80] TrainLoss: 5.4999 | ValRMSE: 9.92px\n",
      "\n",
      "[Epoch 6] Unfreezing backbone...\n",
      "\n",
      "[Epoch 6/80] TrainLoss: 3.7345 | ValRMSE: 10.12px\n",
      "[Epoch 7/80] TrainLoss: 2.3545 | ValRMSE: 10.84px\n",
      "[Epoch 8/80] TrainLoss: 2.4060 | ValRMSE: 9.99px\n",
      "[Epoch 9/80] TrainLoss: 2.2650 | ValRMSE: 10.18px\n",
      "[Epoch 10/80] TrainLoss: 2.0522 | ValRMSE: 9.62px\n",
      "  -> New best model saved! ValRMSE: 9.62px\n",
      "[Epoch 11/80] TrainLoss: 2.1184 | ValRMSE: 9.06px\n",
      "  -> New best model saved! ValRMSE: 9.06px\n",
      "[Epoch 12/80] TrainLoss: 1.8706 | ValRMSE: 8.90px\n",
      "  -> New best model saved! ValRMSE: 8.90px\n",
      "[Epoch 13/80] TrainLoss: 1.6911 | ValRMSE: 8.75px\n",
      "  -> New best model saved! ValRMSE: 8.75px\n",
      "[Epoch 14/80] TrainLoss: 1.4626 | ValRMSE: 8.95px\n",
      "[Epoch 15/80] TrainLoss: 1.3657 | ValRMSE: 8.47px\n",
      "  -> New best model saved! ValRMSE: 8.47px\n",
      "[Epoch 16/80] TrainLoss: 1.5485 | ValRMSE: 8.32px\n",
      "  -> New best model saved! ValRMSE: 8.32px\n",
      "[Epoch 17/80] TrainLoss: 1.4518 | ValRMSE: 8.03px\n",
      "  -> New best model saved! ValRMSE: 8.03px\n",
      "[Epoch 18/80] TrainLoss: 1.3147 | ValRMSE: 8.37px\n",
      "[Epoch 19/80] TrainLoss: 1.3338 | ValRMSE: 8.71px\n",
      "[Epoch 20/80] TrainLoss: 1.2247 | ValRMSE: 9.37px\n",
      "[Epoch 21/80] TrainLoss: 1.3903 | ValRMSE: 8.01px\n",
      "  -> New best model saved! ValRMSE: 8.01px\n",
      "[Epoch 22/80] TrainLoss: 1.2523 | ValRMSE: 8.51px\n",
      "[Epoch 23/80] TrainLoss: 1.2669 | ValRMSE: 8.11px\n",
      "[Epoch 24/80] TrainLoss: 1.2898 | ValRMSE: 7.94px\n",
      "  -> New best model saved! ValRMSE: 7.94px\n",
      "[Epoch 25/80] TrainLoss: 1.3429 | ValRMSE: 8.17px\n",
      "[Epoch 26/80] TrainLoss: 1.2739 | ValRMSE: 8.17px\n",
      "[Epoch 27/80] TrainLoss: 1.1693 | ValRMSE: 9.51px\n",
      "[Epoch 28/80] TrainLoss: 0.9789 | ValRMSE: 8.75px\n",
      "[Epoch 29/80] TrainLoss: 0.9905 | ValRMSE: 8.67px\n",
      "[Epoch 30/80] TrainLoss: 1.1346 | ValRMSE: 7.77px\n",
      "  -> New best model saved! ValRMSE: 7.77px\n",
      "[Epoch 31/80] TrainLoss: 1.0194 | ValRMSE: 8.08px\n",
      "[Epoch 32/80] TrainLoss: 1.0280 | ValRMSE: 8.34px\n",
      "[Epoch 33/80] TrainLoss: 1.0136 | ValRMSE: 8.25px\n",
      "[Epoch 34/80] TrainLoss: 0.9596 | ValRMSE: 8.63px\n",
      "[Epoch 35/80] TrainLoss: 0.9542 | ValRMSE: 8.36px\n",
      "[Epoch 36/80] TrainLoss: 0.9476 | ValRMSE: 8.16px\n",
      "[Epoch 37/80] TrainLoss: 1.0194 | ValRMSE: 7.77px\n",
      "  -> New best model saved! ValRMSE: 7.77px\n",
      "[Epoch 38/80] TrainLoss: 1.0665 | ValRMSE: 7.75px\n",
      "  -> New best model saved! ValRMSE: 7.75px\n",
      "[Epoch 39/80] TrainLoss: 1.1368 | ValRMSE: 7.69px\n",
      "  -> New best model saved! ValRMSE: 7.69px\n",
      "[Epoch 40/80] TrainLoss: 1.0327 | ValRMSE: 9.04px\n",
      "[Epoch 41/80] TrainLoss: 0.9422 | ValRMSE: 7.58px\n",
      "  -> New best model saved! ValRMSE: 7.58px\n",
      "[Epoch 42/80] TrainLoss: 0.8812 | ValRMSE: 9.33px\n",
      "[Epoch 43/80] TrainLoss: 0.8764 | ValRMSE: 7.39px\n",
      "  -> New best model saved! ValRMSE: 7.39px\n",
      "[Epoch 44/80] TrainLoss: 0.9871 | ValRMSE: 7.85px\n",
      "[Epoch 45/80] TrainLoss: 0.9237 | ValRMSE: 7.84px\n",
      "[Epoch 46/80] TrainLoss: 0.8319 | ValRMSE: 8.01px\n",
      "[Epoch 47/80] TrainLoss: 0.7665 | ValRMSE: 8.58px\n",
      "[Epoch 48/80] TrainLoss: 1.2072 | ValRMSE: 6.73px\n",
      "  -> New best model saved! ValRMSE: 6.73px\n",
      "[Epoch 49/80] TrainLoss: 0.9857 | ValRMSE: 7.10px\n",
      "[Epoch 50/80] TrainLoss: 0.8601 | ValRMSE: 7.38px\n",
      "[Epoch 51/80] TrainLoss: 0.8499 | ValRMSE: 7.74px\n",
      "[Epoch 52/80] TrainLoss: 0.8296 | ValRMSE: 6.50px\n",
      "  -> New best model saved! ValRMSE: 6.50px\n",
      "[Epoch 53/80] TrainLoss: 0.8250 | ValRMSE: 6.88px\n",
      "[Epoch 54/80] TrainLoss: 0.7825 | ValRMSE: 7.45px\n",
      "[Epoch 55/80] TrainLoss: 0.7259 | ValRMSE: 7.73px\n",
      "[Epoch 56/80] TrainLoss: 0.8162 | ValRMSE: 7.09px\n",
      "[Epoch 57/80] TrainLoss: 1.1733 | ValRMSE: 8.24px\n",
      "[Epoch 58/80] TrainLoss: 0.9635 | ValRMSE: 7.53px\n",
      "[Epoch 59/80] TrainLoss: 0.8663 | ValRMSE: 7.17px\n",
      "[Epoch 60/80] TrainLoss: 0.8661 | ValRMSE: 6.75px\n",
      "[Epoch 61/80] TrainLoss: 0.8121 | ValRMSE: 7.09px\n",
      "[Epoch 62/80] TrainLoss: 0.8055 | ValRMSE: 6.83px\n",
      "[Epoch 63/80] TrainLoss: 0.7426 | ValRMSE: 7.02px\n",
      "[Epoch 64/80] TrainLoss: 0.7624 | ValRMSE: 7.29px\n",
      "[Epoch 65/80] TrainLoss: 0.7772 | ValRMSE: 7.57px\n",
      "[Epoch 66/80] TrainLoss: 0.7273 | ValRMSE: 6.75px\n",
      "[Epoch 67/80] TrainLoss: 0.7976 | ValRMSE: 7.41px\n",
      "Early stopping after 15 bad epochs\n",
      "Done. Best ValRMSE: 6.50px | Saved to /Users/aryan078/Desktop/CHD_project/stage2_finetuned_best.pth\n"
     ]
    }
   ],
   "source": [
    "# Dataloaders\n",
    "train_loader = DataLoader(KeypointDataset(final_train_df, train_tf_flip),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate,\n",
    "                          num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(KeypointDataset(final_val_df, val_tf),\n",
    "                        batch_size=BATCH_SIZE, collate_fn=custom_collate,\n",
    "                        num_workers=0, pin_memory=True)\n",
    "\n",
    "# Model init + load Stage 1\n",
    "model = HipNet().to(device)\n",
    "state_dict = torch.load(STAGE1_MODEL_PATH, map_location=device)\n",
    "filtered_state_dict = {k: v for k, v in state_dict.items() if k in model.state_dict()}\n",
    "model.load_state_dict(filtered_state_dict, strict=False)\n",
    "\n",
    "print(f\"Loaded Stage 1 weights from {STAGE1_MODEL_PATH}\")\n",
    "\n",
    "# Freeze backbone for warm-up\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
    "\n",
    "best_rmse = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(EPOCHS_STAGE2):\n",
    "\n",
    "    if epoch == WARMUP_EPOCHS:\n",
    "        print(f\"\\n[Epoch {epoch+1}] Unfreezing backbone...\\n\")\n",
    "        for p in model.backbone.parameters():\n",
    "            p.requires_grad = True\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, (imgs, hmaps, gt_coords, _) in enumerate(train_loader):\n",
    "        imgs, hmaps = imgs.to(device), hmaps.to(device)\n",
    "        hmap_pred, _, _ = model(imgs)\n",
    "        coords_pred = soft_argmax_2d(hmap_pred)\n",
    "        gt_coords_t = torch.tensor(np.stack(gt_coords), dtype=torch.float32).to(device)\n",
    "        loss = weighted_mse_loss(hmap_pred, hmaps) + 0.05 * coord_loss_fn(coords_pred, gt_coords_t)\n",
    "        loss.backward()\n",
    "        if (step + 1) % ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_rmse_list = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, _, gt_coords, names in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            hmap_pred, _, _ = model(imgs)\n",
    "            coords_pred = soft_argmax_2d(hmap_pred).cpu().numpy()\n",
    "            for b in range(len(gt_coords)):\n",
    "                rmse = np.sqrt(((coords_pred[b] - gt_coords[b]) ** 2).sum(axis=1)).mean()\n",
    "                val_rmse_list.append(rmse)\n",
    "    avg_rmse = np.mean(val_rmse_list)\n",
    "    print(f\"[Epoch {epoch+1}/{EPOCHS_STAGE2}] TrainLoss: {train_loss/len(train_loader):.4f} | ValRMSE: {avg_rmse:.2f}px\")\n",
    "\n",
    "    if avg_rmse < best_rmse:\n",
    "        best_rmse = avg_rmse\n",
    "        torch.save(model.state_dict(), STAGE2_MODEL_PATH)\n",
    "        print(f\"  -> New best model saved! ValRMSE: {best_rmse:.2f}px\")\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= PATIENCE:\n",
    "            print(f\"Early stopping after {PATIENCE} bad epochs\")\n",
    "            break\n",
    "\n",
    "print(f\"Done. Best ValRMSE: {best_rmse:.2f}px | Saved to {STAGE2_MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ 1 missing files will be dropped:\n",
      "               image_name                                         image_path\n",
      "100  Radiograph4_hip0.jpg  /Users/aryan078/Desktop/CHD_project/data/u110/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CPU Finetune][Epoch 1/8] TrainLoss 1.7167 | ValRMSE 3.88\n",
      "  -> Saved new best model with RMSE 3.88\n",
      "[CPU Finetune][Epoch 2/8] TrainLoss 1.7150 | ValRMSE 4.89\n",
      "[CPU Finetune][Epoch 3/8] TrainLoss 1.9894 | ValRMSE 4.36\n",
      "[CPU Finetune][Epoch 4/8] TrainLoss 1.6133 | ValRMSE 4.95\n",
      "[CPU Finetune][Epoch 5/8] TrainLoss 1.5816 | ValRMSE 3.48\n",
      "  -> Saved new best model with RMSE 3.48\n",
      "[CPU Finetune][Epoch 6/8] TrainLoss 1.4329 | ValRMSE 3.51\n",
      "[CPU Finetune][Epoch 7/8] TrainLoss 1.2541 | ValRMSE 4.83\n",
      "[CPU Finetune][Epoch 8/8] TrainLoss 1.2707 | ValRMSE 3.21\n",
      "  -> Saved new best model with RMSE 3.21\n",
      "Best Val RMSE: 3.21 px, saved at /Users/aryan078/Desktop/CHD_project/stage2_finetuned_cpu_best.pth\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "GT99_IMG_DIR = PROJECT_ROOT / \"data/annotation_batch\"\n",
    "GT99_CSV = GT99_IMG_DIR / \"keypoints_normalized_FIXED.csv\"\n",
    "VAL_SPLIT_GT99_CSV = PROJECT_ROOT / \"outputs\" / \"csv\" / \"val_split_gt99.csv\"\n",
    "U110_IMG_DIR = PROJECT_ROOT / \"data/u110\"\n",
    "U110_CSV = U110_IMG_DIR / \"keypoints_normalized_from_annotations5.csv\"\n",
    "\n",
    "STAGE2_MODEL_PATH = PROJECT_ROOT / \"outputs\" / \"model\" / \"stage2_finetuned.pth\"\n",
    "STAGE2_CPU_PATH = PROJECT_ROOT / \"outputs\" / \"model\" / \"stage2_finetuned_cpu_best.pth\"\n",
    "VAL_VIZ_DIR = PROJECT_ROOT / \"outputs\" / \"visualizations\" / \"stage2_cpu_val_viz\"\n",
    "VAL_VIZ_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Hyperparameters\n",
    "IMG_SIZE = 512\n",
    "HEATMAP_SIZE = 256\n",
    "SIGMA = 8\n",
    "BATCH_SIZE = 1\n",
    "ACCUM_STEPS = 4\n",
    "LR = 1e-4\n",
    "EPOCHS = 8\n",
    "WARMUP_EPOCHS = 2\n",
    "HEAT_W = 1.0\n",
    "COORD_W = 0.12\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "# Utils\n",
    "def generate_heatmaps(keypoints, heatmap_size=HEATMAP_SIZE, img_size=IMG_SIZE, sigma=SIGMA):\n",
    "    num_kps = len(keypoints)\n",
    "    heatmaps = np.zeros((num_kps, heatmap_size, heatmap_size), dtype=np.float32)\n",
    "    for i, (x, y) in enumerate(keypoints):\n",
    "        x_h = x * (heatmap_size / img_size)\n",
    "        y_h = y * (heatmap_size / img_size)\n",
    "        xx, yy = np.meshgrid(np.arange(heatmap_size), np.arange(heatmap_size))\n",
    "        heatmaps[i] = np.exp(-((xx - x_h)**2 + (yy - y_h)**2) / (2 * sigma**2))\n",
    "    return heatmaps\n",
    "\n",
    "def soft_argmax_2d(heatmaps, output_size=IMG_SIZE, heatmap_size=HEATMAP_SIZE):\n",
    "    N, K, H, W = heatmaps.shape\n",
    "    flat = heatmaps.view(N, K, -1)\n",
    "    flat = torch.softmax(flat, dim=-1)\n",
    "    coords_x = torch.arange(W).repeat(H, 1).reshape(-1).float().to(flat.device)\n",
    "    coords_y = torch.arange(H).repeat_interleave(W).float().to(flat.device)\n",
    "    xs = torch.sum(flat * coords_x, dim=-1)\n",
    "    ys = torch.sum(flat * coords_y, dim=-1)\n",
    "    scale = output_size / float(heatmap_size)\n",
    "    coords = torch.stack([xs * scale, ys * scale], dim=-1)\n",
    "    return coords.view(N, K, 2)\n",
    "\n",
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transforms = transforms\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = cv2.cvtColor(cv2.imread(row[\"image_path\"]), cv2.COLOR_BGR2RGB)\n",
    "        keypoints = np.array([\n",
    "            (row['L_FHC_x'] * IMG_SIZE, row['L_FHC_y'] * IMG_SIZE),\n",
    "            (row['R_FHC_x'] * IMG_SIZE, row['R_FHC_y'] * IMG_SIZE),\n",
    "            (row['L_CAR_x'] * IMG_SIZE, row['L_CAR_y'] * IMG_SIZE),\n",
    "            (row['R_CAR_x'] * IMG_SIZE, row['R_CAR_y'] * IMG_SIZE),\n",
    "        ], dtype=np.float32)\n",
    "        if self.transforms:\n",
    "            aug = self.transforms(image=image, keypoints=keypoints)\n",
    "            image, keypoints = aug['image'], aug['keypoints']\n",
    "        heatmaps = generate_heatmaps(keypoints.tolist())\n",
    "        return image, torch.tensor(heatmaps), np.array(keypoints, dtype=np.float32), row[\"image_name\"]\n",
    "\n",
    "def custom_collate(batch):\n",
    "    imgs, hmaps, kps, names = zip(*batch)\n",
    "    return torch.stack(imgs), torch.stack(hmaps), list(kps), list(names)\n",
    "\n",
    "class HorizontalFlipWithKeypointSwap(A.DualTransform):\n",
    "    def apply(self, image, **params): return cv2.flip(image, 1)\n",
    "    def apply_to_keypoints(self, keypoints, **params):\n",
    "        keypoints[:, 0] = IMG_SIZE - keypoints[:, 0]\n",
    "        keypoints[[0, 1]] = keypoints[[1, 0]]\n",
    "        keypoints[[2, 3]] = keypoints[[3, 2]]\n",
    "        return keypoints\n",
    "    def get_transform_init_args_names(self): return ()\n",
    "\n",
    "train_tf = A.Compose([\n",
    "    HorizontalFlipWithKeypointSwap(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.05, rotate_limit=7, border_mode=cv2.BORDER_CONSTANT, p=0.9),\n",
    "    A.CLAHE(clip_limit=2.0, tile_grid_size=(8,8), p=0.25),\n",
    "    A.RandomGamma(gamma_limit=(80,120), p=0.25),\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], keypoint_params=A.KeypointParams(format='xy'))\n",
    "\n",
    "val_tf = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], keypoint_params=A.KeypointParams(format='xy'))\n",
    "\n",
    "class HipNet(nn.Module):\n",
    "    def __init__(self, num_keypoints=4):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\"mobilenetv3_large_100\", pretrained=True, features_only=True)\n",
    "        in_ch = self.backbone.feature_info[-1]['num_chs']\n",
    "        self.kp_head = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, num_keypoints, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)[-1]\n",
    "        hmap = self.kp_head(torch.nn.functional.interpolate(feats, size=(HEATMAP_SIZE, HEATMAP_SIZE)))\n",
    "        return hmap, None, None\n",
    "\n",
    "weights = torch.tensor([2.0, 2.0, 1.0, 1.0])\n",
    "def weighted_mse_loss(pred, target):\n",
    "    return (((pred - target)**2) * weights.view(1,-1,1,1).to(pred.device)).mean()\n",
    "coord_loss_fn = nn.MSELoss()\n",
    "\n",
    "# Prepare Data (same split as Stage 2)\n",
    "df_gt99 = pd.read_csv(GT99_CSV)\n",
    "df_u110 = pd.read_csv(U110_CSV)\n",
    "df_gt99['image_path'] = df_gt99['image_name'].apply(lambda n: str(GT99_IMG_DIR / n))\n",
    "df_u110['image_path'] = df_u110['image_name'].apply(lambda n: str(U110_IMG_DIR / n))\n",
    "def filter_missing(df):\n",
    "    mask = df['image_path'].apply(lambda p: Path(p).exists())\n",
    "    missing = df[~mask]\n",
    "    if len(missing) > 0:\n",
    "        print(f\"⚠ {len(missing)} missing files will be dropped:\")\n",
    "        print(missing[['image_name', 'image_path']].head(10))\n",
    "    return df[mask]\n",
    "\n",
    "df_gt99 = filter_missing(df_gt99)\n",
    "df_u110 = filter_missing(df_u110)\n",
    "\n",
    "val_gt99_names = set(pd.read_csv(VAL_SPLIT_GT99_CSV)['image_name'])\n",
    "train_u110_df, val_u110_df = train_test_split(df_u110, test_size=0.2, random_state=42)\n",
    "val_u110_names = set(val_u110_df['image_name'])\n",
    "\n",
    "final_val_names = val_gt99_names.union(val_u110_names)\n",
    "df_combined = pd.concat([df_gt99, df_u110], ignore_index=True)\n",
    "final_train_df = df_combined[~df_combined['image_name'].isin(final_val_names)]\n",
    "final_val_df = df_combined[df_combined['image_name'].isin(final_val_names)]\n",
    "\n",
    "train_loader = DataLoader(KeypointDataset(final_train_df, train_tf), batch_size=BATCH_SIZE,\n",
    "                          shuffle=True, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(KeypointDataset(final_val_df, val_tf), batch_size=1,\n",
    "                        collate_fn=custom_collate)\n",
    "\n",
    "# Load Model from Stage2\n",
    "model = HipNet().to(device)\n",
    "model.load_state_dict(torch.load(STAGE2_MODEL_PATH, map_location=device))\n",
    "\n",
    "# Freeze backbone for warmup\n",
    "for p in model.backbone.parameters(): p.requires_grad = False\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                               lr=LR, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                       factor=0.5, patience=2, min_lr=1e-6)\n",
    "best_rmse = float('inf')\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    if epoch == WARMUP_EPOCHS:\n",
    "        for p in model.backbone.parameters(): p.requires_grad = True\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    for step, (imgs, hmaps, gt_coords, _) in enumerate(train_loader):\n",
    "        imgs, hmaps = imgs.to(device), hmaps.to(device)\n",
    "        hmap_pred, _, _ = model(imgs)\n",
    "        coords_pred = soft_argmax_2d(hmap_pred)\n",
    "        gt_coords_t = torch.tensor(np.stack(gt_coords), dtype=torch.float32).to(device)\n",
    "        loss = HEAT_W * weighted_mse_loss(hmap_pred, hmaps) + COORD_W * coord_loss_fn(coords_pred, gt_coords_t)\n",
    "        loss.backward()\n",
    "        if (step+1) % ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validate with TTA\n",
    "    model.eval()\n",
    "    val_rmse_list = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, _, gt_coords, names in val_loader:\n",
    "            # Original\n",
    "            hmap_pred, _, _ = model(imgs.to(device))\n",
    "            coords1 = soft_argmax_2d(hmap_pred).cpu().numpy()\n",
    "            # Flipped\n",
    "            imgs_flip = torch.flip(imgs, dims=[3])\n",
    "            hmap_flip, _, _ = model(imgs_flip.to(device))\n",
    "            hmap_flip = torch.flip(hmap_flip, dims=[3])\n",
    "            coords2 = soft_argmax_2d(hmap_flip).cpu().numpy()\n",
    "            coords2[:, [0,1]] = coords2[:, [1,0]]\n",
    "            coords2[:, [2,3]] = coords2[:, [3,2]]\n",
    "            coords_avg = (coords1 + coords2) / 2.0\n",
    "            for b in range(len(gt_coords)):\n",
    "                rmse = np.sqrt(((coords_avg[b] - gt_coords[b])**2).sum(axis=1)).mean()\n",
    "                val_rmse_list.append(rmse)\n",
    "\n",
    "    avg_rmse = np.mean(val_rmse_list)\n",
    "    print(f\"[CPU Finetune][Epoch {epoch+1}/{EPOCHS}] TrainLoss {train_loss/len(train_loader):.4f} | ValRMSE {avg_rmse:.2f}\")\n",
    "    scheduler.step(avg_rmse)\n",
    "\n",
    "    if avg_rmse < best_rmse:\n",
    "        best_rmse = avg_rmse\n",
    "        torch.save(model.state_dict(), STAGE2_CPU_PATH)\n",
    "        print(f\"  -> Saved new best model with RMSE {best_rmse:.2f}\")\n",
    "\n",
    "print(f\"Best Val RMSE: {best_rmse:.2f} px, saved at {STAGE2_CPU_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Visualization images saved to: /Users/aryan078/Desktop/CHD_project/stage2_cpu_val_viz\n"
     ]
    }
   ],
   "source": [
    "# Visualization & Save for Stage2 CPU Best\n",
    "model = HipNet().to(device)\n",
    "model.load_state_dict(torch.load(STAGE2_CPU_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, _, gt_coords, names in val_loader:\n",
    "        out = model(imgs.to(device))\n",
    "        if isinstance(out, tuple):\n",
    "            hmap_pred = out[0]\n",
    "        else:\n",
    "            hmap_pred = out\n",
    "\n",
    "        coords_pred = soft_argmax_2d(hmap_pred).cpu().numpy()[0]\n",
    "\n",
    "        img_np = imgs[0].cpu().permute(1, 2, 0).numpy()\n",
    "        img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "\n",
    "        plt.figure()\n",
    "        plt.imshow(img_np)\n",
    "        plt.scatter(gt_coords[0][:,0], gt_coords[0][:,1], c='g', s=15, label=\"GT\")\n",
    "        plt.scatter(coords_pred[:,0], coords_pred[:,1], c='r', s=15, label=\"Pred\")\n",
    "        plt.legend()\n",
    "        plt.title(names[0])\n",
    "        plt.axis('off')\n",
    "\n",
    "        out_path = VAL_VIZ_DIR / f\"{names[0]}_pred_vs_gt.png\"\n",
    "        plt.savefig(out_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "print(f\"✅ Visualization images saved to: {VAL_VIZ_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Missing image: /Users/aryan078/Desktop/CHD_project/data/check/Radiograph02_hip0.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@1829.234] global loadsave.cpp:275 findDecoder imread_('/Users/aryan078/Desktop/CHD_project/data/check/Radiograph02_hip0.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: /Users/aryan078/Desktop/CHD_project/outputs/csv/val_predictions_with_metrics.csv\n",
      "✅ Annotated images saved to: /Users/aryan078/Desktop/CHD_project/outputs/visualizations/annotated_preds\n",
      "Confusion Matrix:\n",
      " [[ 7  6]\n",
      " [ 4 24]]\n",
      "Accuracy   : 0.756\n",
      "Sensitivity: 0.857\n",
      "Specificity: 0.538\n",
      " Plots saved to: /Users/aryan078/Desktop/CHD_project/outputs/visualizations/evaluation_plots\n"
     ]
    }
   ],
   "source": [
    "# STAGE 3: EVALUATION (with extra plots & visualizations)\n",
    "\n",
    "import math, cv2, torch, timm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# config\n",
    "IMG_SIZE = 512\n",
    "HEATMAP_SIZE = 256\n",
    "SIGMA = 8\n",
    "ANGLE_THR = 105.0\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# paths\n",
    "MODEL_PATH = PROJECT_ROOT / \"outputs\" / \"model\" / \"stage2_finetuned_cpu_best.pth\"\n",
    "input_dir = PROJECT_ROOT / \"data\" / \"check\"\n",
    "gt_csv_path = PROJECT_ROOT / \"outputs\" / \"csv\" / \"val_split_combined.csv\" \n",
    "out_csv_path = PROJECT_ROOT / \"outputs\" / \"csv\" / \"val_predictions_with_metrics.csv\"\n",
    "annot_dir = PROJECT_ROOT / \"outputs\" / \"visualizations\" / \"annotated_preds\"\n",
    "plots_dir = PROJECT_ROOT / \"outputs\" / \"visualizations\" / \"evaluation_plots\"\n",
    "annot_dir.mkdir(exist_ok=True)\n",
    "plots_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# utilities\n",
    "def compute_norberg_angles(coords):\n",
    "    def angle(a, b, c):\n",
    "        AB = a - b\n",
    "        CB = c - b\n",
    "        dot = np.dot(AB, CB)\n",
    "        denom = np.linalg.norm(AB) * np.linalg.norm(CB)\n",
    "        return math.degrees(math.acos(np.clip(dot / denom, -1, 1))) if denom > 0 else 0\n",
    "    angL = angle(coords[2], coords[0], coords[1])\n",
    "    angR = angle(coords[3], coords[1], coords[0])\n",
    "    return angL, angR\n",
    "\n",
    "def soft_argmax_2d(hmaps):\n",
    "    N, K, H, W = hmaps.shape\n",
    "    flat = hmaps.view(N, K, -1)\n",
    "    flat = torch.softmax(flat, dim=-1)\n",
    "    coords_x = torch.arange(W).repeat(H, 1).reshape(-1).float().to(flat.device)\n",
    "    coords_y = torch.arange(H).repeat_interleave(W).float().to(flat.device)\n",
    "    xs = torch.sum(flat * coords_x, dim=-1)\n",
    "    ys = torch.sum(flat * coords_y, dim=-1)\n",
    "    scale = IMG_SIZE / float(HEATMAP_SIZE)\n",
    "    coords = torch.stack([xs * scale, ys * scale], dim=-1)\n",
    "    return coords.view(N, K, 2)\n",
    "\n",
    "def draw_angle_arc(img, center, p1, p2, angle_val, color=(0,255,0), radius=50, thickness=2):\n",
    "    def _angle(v1, v2):\n",
    "        dot = np.dot(v1, v2)\n",
    "        return math.degrees(math.acos(np.clip(dot / (np.linalg.norm(v1)*np.linalg.norm(v2)), -1, 1)))\n",
    "    v1 = np.array(p1) - np.array(center)\n",
    "    v2 = np.array(p2) - np.array(center)\n",
    "    ang = _angle(v1, v2)\n",
    "    start_angle = math.degrees(math.atan2(-v1[1], v1[0]))\n",
    "    end_angle = start_angle + ang\n",
    "    cv2.ellipse(img, (int(center[0]), int(center[1])), (radius, radius), 0, start_angle, end_angle, color, thickness)\n",
    "    label_pos = (int(center[0] + radius/2), int(center[1] - radius/2))\n",
    "    cv2.putText(img, f\"{angle_val:.1f}\", label_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)\n",
    "    return img\n",
    "\n",
    "# transforms\n",
    "val_tf = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], keypoint_params=A.KeypointParams(format='xy'))\n",
    "\n",
    "# model\n",
    "class HipNet(nn.Module):\n",
    "    def __init__(self, num_keypoints=4):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\"mobilenetv3_large_100\", pretrained=True, features_only=True)\n",
    "        in_ch = self.backbone.feature_info[-1]['num_chs']\n",
    "        self.kp_head = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, num_keypoints, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)[-1]\n",
    "        hmap = self.kp_head(torch.nn.functional.interpolate(feats, size=(HEATMAP_SIZE, HEATMAP_SIZE)))\n",
    "        return hmap\n",
    "\n",
    "model = HipNet().to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# inference\n",
    "gt_df = pd.read_csv(gt_csv_path)\n",
    "results = []\n",
    "\n",
    "for _, row in gt_df.iterrows():\n",
    "    img_path = Path(input_dir) / row[\"image_name\"]\n",
    "    img_bgr = cv2.imread(str(img_path))\n",
    "    if img_bgr is None:\n",
    "        print(f\"⚠ Missing image: {img_path}\")\n",
    "        continue\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    tf_img = val_tf(image=img_rgb, keypoints=[(0,0)]*4)\n",
    "    img_tensor = tf_img[\"image\"].unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        coords1 = soft_argmax_2d(model(img_tensor)).cpu().numpy()\n",
    "        img_flip = torch.flip(img_tensor, dims=[3])\n",
    "        hmap_flip = model(img_flip)\n",
    "        hmap_flip = torch.flip(hmap_flip, dims=[3])\n",
    "        coords2 = soft_argmax_2d(hmap_flip).cpu().numpy()\n",
    "        coords2[:, [0,1]] = coords2[:, [1,0]]\n",
    "        coords2[:, [2,3]] = coords2[:, [3,2]]\n",
    "        coords_avg = (coords1 + coords2) / 2.0\n",
    "        coords_avg = coords_avg[0]\n",
    "\n",
    "    pred_L, pred_R = compute_norberg_angles(coords_avg)\n",
    "    pred_class = \"Normal\" if min(pred_L, pred_R) >= ANGLE_THR else \"Dysplasia\"\n",
    "\n",
    "    gt_coords = np.array([\n",
    "        (row['L_FHC_x']*IMG_SIZE, row['L_FHC_y']*IMG_SIZE),\n",
    "        (row['R_FHC_x']*IMG_SIZE, row['R_FHC_y']*IMG_SIZE),\n",
    "        (row['L_CAR_x']*IMG_SIZE, row['L_CAR_y']*IMG_SIZE),\n",
    "        (row['R_CAR_x']*IMG_SIZE, row['R_CAR_y']*IMG_SIZE),\n",
    "    ])\n",
    "    gt_L, gt_R = compute_norberg_angles(gt_coords)\n",
    "    gt_class = \"Normal\" if min(gt_L, gt_R) >= ANGLE_THR else \"Dysplasia\"\n",
    "\n",
    "    results.append({\n",
    "        \"filename\": row[\"image_name\"],\n",
    "        \"GT_angle_L\": gt_L, \"GT_angle_R\": gt_R, \"GT_class\": gt_class,\n",
    "        \"Pred_angle_L\": pred_L, \"Pred_angle_R\": pred_R, \"Pred_class\": pred_class\n",
    "    })\n",
    "\n",
    "    img_annot = img_rgb.copy()\n",
    "    # GT arcs\n",
    "    cv2.line(img_annot, tuple(map(int, gt_coords[0])), tuple(map(int, gt_coords[2])), (0,255,0), 2)\n",
    "    cv2.line(img_annot, tuple(map(int, gt_coords[0])), tuple(map(int, gt_coords[1])), (0,255,0), 2)\n",
    "    img_annot = draw_angle_arc(img_annot, gt_coords[0], gt_coords[2], gt_coords[1], gt_L, (0,255,0))\n",
    "    cv2.line(img_annot, tuple(map(int, gt_coords[1])), tuple(map(int, gt_coords[3])), (0,255,0), 2)\n",
    "    cv2.line(img_annot, tuple(map(int, gt_coords[1])), tuple(map(int, gt_coords[0])), (0,255,0), 2)\n",
    "    img_annot = draw_angle_arc(img_annot, gt_coords[1], gt_coords[3], gt_coords[0], gt_R, (0,255,0))\n",
    "    # Pred arcs\n",
    "    cv2.line(img_annot, tuple(map(int, coords_avg[0])), tuple(map(int, coords_avg[2])), (0,0,255), 2)\n",
    "    cv2.line(img_annot, tuple(map(int, coords_avg[0])), tuple(map(int, coords_avg[1])), (0,0,255), 2)\n",
    "    img_annot = draw_angle_arc(img_annot, coords_avg[0], coords_avg[2], coords_avg[1], pred_L, (0,0,255))\n",
    "    cv2.line(img_annot, tuple(map(int, coords_avg[1])), tuple(map(int, coords_avg[3])), (0,0,255), 2)\n",
    "    cv2.line(img_annot, tuple(map(int, coords_avg[1])), tuple(map(int, coords_avg[0])), (0,0,255), 2)\n",
    "    img_annot = draw_angle_arc(img_annot, coords_avg[1], coords_avg[3], coords_avg[0], pred_R, (0,0,255))\n",
    "    # Keypoints\n",
    "    for (x,y) in gt_coords: cv2.circle(img_annot, (int(x),int(y)), 5, (0,255,0), -1)\n",
    "    for (x,y) in coords_avg: cv2.circle(img_annot, (int(x),int(y)), 5, (0,0,255), -1)\n",
    "    # Labels\n",
    "    cv2.putText(img_annot, f\"GT: {gt_class}\", (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "    cv2.putText(img_annot, f\"PR: {pred_class}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,255), 2)\n",
    "\n",
    "    save_path = annot_dir / f\"{row['image_name'].rsplit('.',1)[0]}_annot.png\"\n",
    "    cv2.imwrite(str(save_path), cv2.cvtColor(img_annot, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "# metrics & plots\n",
    "res_df = pd.DataFrame(results)\n",
    "res_df.to_csv(out_csv_path, index=False)\n",
    "print(f\"✅ Results saved to: {out_csv_path}\")\n",
    "print(f\"✅ Annotated images saved to: {annot_dir}\")\n",
    "\n",
    "label_map = {\"Normal\": 0, \"Dysplasia\": 1}\n",
    "y_true = res_df[\"GT_class\"].map(label_map)\n",
    "y_pred = res_df[\"Pred_class\"].map(label_map)\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "accuracy = (TP + TN) / cm.sum()\n",
    "sensitivity = TP / (TP + FN) if (TP+FN) > 0 else 0\n",
    "specificity = TN / (TN + FP) if (TN+FP) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(f\"Accuracy   : {accuracy:.3f}\")\n",
    "print(f\"Sensitivity: {sensitivity:.3f}\")\n",
    "print(f\"Specificity: {specificity:.3f}\")\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\",\"Dysplasia\"],\n",
    "            yticklabels=[\"Normal\",\"Dysplasia\"])\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plots_dir / \"confusion_matrix.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Bar chart for metrics\n",
    "metric_names = [\"Accuracy\", \"Sensitivity\", \"Specificity\"]\n",
    "metric_values = [accuracy, sensitivity, specificity]\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.barplot(x=metric_names, y=metric_values, palette=\"viridis\")\n",
    "plt.ylim(0,1)\n",
    "for i, val in enumerate(metric_values):\n",
    "    plt.text(i, val+0.01, f\"{val:.2f}\", ha='center', va='bottom')\n",
    "plt.ylabel(\"Score\"); plt.title(\"Evaluation Metrics\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plots_dir / \"metrics_barplot.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Histogram of angles (GT vs Pred L/R)\n",
    "for side in [\"L\", \"R\"]:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(res_df[f\"GT_angle_{side}\"], bins=20, alpha=0.5, label=f\"GT {side}\")\n",
    "    plt.hist(res_df[f\"Pred_angle_{side}\"], bins=20, alpha=0.5, label=f\"Pred {side}\")\n",
    "    plt.axvline(ANGLE_THR, color='red', linestyle='--', label=\"Threshold\")\n",
    "    plt.xlabel(\"Angle (deg)\"); plt.ylabel(\"Count\")\n",
    "    plt.title(f\"Norberg Angle Distribution - {side}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plots_dir / f\"angle_dist_{side}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "print(f\" Plots saved to: {plots_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
